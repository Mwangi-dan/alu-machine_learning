# Optimization
Here, I learn about
- Hyperparameters
- Normalization of input data
- Saddle point
- Stochastic gradient descent?
- Mini-batch gradient descent?
- Moving average
- RMSProp
- Adam optimization
- Learning rate decay
- Batch normalization